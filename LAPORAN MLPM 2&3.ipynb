{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum II & III Machine Learning**"
      ],
      "metadata": {
        "id": "-Cr3-KWzf6GB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=================================================\n",
        "\n",
        "Nama  : Veronnica Noer Fhaeza\n",
        "\n",
        "NPM : F1F022012\n",
        "\n",
        "================================================="
      ],
      "metadata": {
        "id": "8SouFcQkgDTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latar Belakang"
      ],
      "metadata": {
        "id": "GTpLr55TgOml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam analisis data dan pembelajaran mesin, penentuan metode klasifikasi maupun prediksi yang tepat merupakan langkah fundamental untuk mencapai hasil analisis yang akurat. Peningkatan volume dan kompleksitas data telah mendorong adopsi algoritma yang efektif seperti Decision Tree, Regression Tree, dan K-Nearest Neighbors (KNN). Decision Tree (DT) merupakan salah satu teknik pemodelan yang berharga karena kemampuannya dalam memberikan struktur pohon yang visual dan mudah diinterpretasikan, yang memfasilitasi pemahaman terhadap alur pengambilan keputusan yang logis dan bertahap (Sari & Ramadhan, 2021). Teknik ini bekerja dengan memecah data secara rekursif melalui simpul-simpul, di mana proses pemecahan dapat menghasilkan pohon biner atau struktur yang lebih kompleks, bergantung pada tipe dan jumlah kategori fitur yang digunakan. Berbagai algoritma pembangun Decision Tree seperti ID3, C4.5, dan CART terus menjadi fokus penelitian untuk meningkatkan efisiensi dan akurasi model pada berbagai kasus, seperti yang diterapkan dalam klasifikasi data akademik atau bisnis (Santoso & Arifin, 2020).\n",
        "\n",
        "Sementara itu, metode K-Nearest Neighbors (KNN) dikenal sebagai algoritma yang sederhana namun tetap efektif dan sering digunakan untuk tugas klasifikasi. KNN bekerja berdasarkan prinsip kemiripan jarak (similarity) dan termasuk dalam kategori instance-based learning. Metode ini mengklasifikasikan objek data baru dengan cara mengidentifikasi sejumlah $K$ tetangga terdekatnya dalam ruang fitur, kemudian menetapkan kelas mayoritas dari tetangga tersebut sebagai hasil prediksi (Hidayah, Setiadi, & Ramadhani, 2022). Penelitian terkait KNN seringkali mengeksplorasi optimasi dalam penentuan nilai $K$ yang optimal dan penanganan data yang tidak seimbang (imbalanced data) untuk mengurangi sensitivitasnya terhadap noise dan meningkatkan performa prediksi. Pemahaman komprehensif mengenai karakteristik, kelebihan, dan cara kerja dari Decision Tree, Regression Tree, dan KNN sangat esensial bagi praktisi data untuk dapat memilih dan mengaplikasikan teknik analisis yang paling sesuai dan memberikan hasil yang optimal sesuai dengan karakteristik data yang dihadapi."
      ],
      "metadata": {
        "id": "ueJxTJDKgf2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rumusan Masalah"
      ],
      "metadata": {
        "id": "al9asCQfgiIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rumusan masalah pada pertemuan ini adalah:\n",
        "\n",
        "1. Apa itu konsep dasar algoritma Decision Tree?\n",
        "2. Bagaimana membedakan antara Classification Tree dan Regression Tree?\n",
        "3. Bagaimana menerapkan Decision Tree untuk klasifikasi?\n",
        "4. Bagaimana melakukan evaluasi model Decision Tree menggunakan confusion matrix dan accuracy score?\n",
        "5. Bagaimana menginterpretasikan hasil model dalam bentuk visualisasi pohon keputusan?\n",
        "6. Bagaimana melatih kemampuan analisis melalui latihan mandiri menggunakan dataset lain?"
      ],
      "metadata": {
        "id": "dA1uuaxSgmsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tujuan Penelitian"
      ],
      "metadata": {
        "id": "ubbw5NpxgnSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tujuan pada pertemuan ini adalah sebagai berikut:\n",
        "\n",
        "1. Untuk menjelaskan konsep dasar algoritma Decision Tree.\n",
        "2. Untuk membedakan antara Classification Tree dan Regression Tree.\n",
        "3. Untuk menerapkan algoritma Decision Tree dalam klasifikasi data.\n",
        "4. Untuk mengevaluasi model Decision Tree menggunakan confusion matrix dan accuracy score.\n",
        "5. Untuk menginterpretasikan hasil model melalui visualisasi pohon keputusan.\n",
        "6. Untuk melatih kemampuan analisis melalui penerapan Decision Tree pada dataset lain."
      ],
      "metadata": {
        "id": "T9C2RleViPU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mengimpor _Libraries_"
      ],
      "metadata": {
        "id": "pUHqIMnLjZUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikut merupakan _library_ yang akan digunakan dalam _notebook_ ini:"
      ],
      "metadata": {
        "id": "Csdn-01pldXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feature_engine"
      ],
      "metadata": {
        "id": "MsXrAovWliJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72800ea-6c07-4378-d8f5-69cbba98232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from feature_engine) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->feature_engine) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->feature_engine) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.0->feature_engine) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.1->feature_engine) (1.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.11.1->feature_engine) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->feature_engine) (1.17.0)\n",
            "Downloading feature_engine-1.9.3-py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "ZJGvb4aUloTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memuat Data"
      ],
      "metadata": {
        "id": "opbNdjiclzpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menunjukkan cara kerja Decision Tree, kita akan menggunakan dataset tentang Breast Cancer (Diagnostic) untuk memprediksi apakah sebuah tumor bersifat ganas (malignant) atau jinak (benign).\n",
        "(https://raw.githubusercontent.com/Royallist/DATA-MACHINE-LEARNING/refs/heads/main/data.csv)."
      ],
      "metadata": {
        "id": "zJHTTtbMl4TX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_df = pd.read_csv('https://raw.githubusercontent.com/Royallist/DATA-MACHINE-LEARNING/refs/heads/main/data.csv')\n",
        "bc_df"
      ],
      "metadata": {
        "id": "qCmB-I2axnW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop Column"
      ],
      "metadata": {
        "id": "dRo1x_USyZgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kolom id adalah identifier unik, tidak punya hubungan dengan diagnosis sehingga untuk kolom id kita hapus saja"
      ],
      "metadata": {
        "id": "vVwSkr979W7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'id' in bc_df.columns:\n",
        "    bc_df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "bc_df"
      ],
      "metadata": {
        "id": "4gS9i4McydkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah itu, disini saya akan menghapus kolom *Unnamed 32* karena tidak berisi data dan dapat mengganggu *training model*"
      ],
      "metadata": {
        "id": "dVUnHXIs-u0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# menghapus kolom Unnamed 32\n",
        "if 'Unnamed: 32' in bc_df.columns:\n",
        "    bc_df.drop('Unnamed: 32', axis=1, inplace=True)\n",
        "\n",
        "    print(\"Kolom setelah dihapus:\")\n",
        "print(list(bc_df.columns))"
      ],
      "metadata": {
        "id": "NKJyewWV-Ots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Data Eksploratif"
      ],
      "metadata": {
        "id": "JzZ7sijEziaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cek struktur dataset\n",
        "bc_df.info()"
      ],
      "metadata": {
        "id": "sglZI7NQzlL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistik deskriptif\n",
        "bc_df.describe()"
      ],
      "metadata": {
        "id": "f31UC_Ph1dp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Feature Engineering_"
      ],
      "metadata": {
        "id": "iP8c4Pe71nIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A - Missing Value"
      ],
      "metadata": {
        "id": "IfITWVaq1xss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_df.isnull().sum()"
      ],
      "metadata": {
        "id": "wqaATWmt1yLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B - Kardinalitas"
      ],
      "metadata": {
        "id": "A0jNs-H-2Fp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C- Splitting Data"
      ],
      "metadata": {
        "id": "1VYTSJ142JIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = bc_df.drop(['diagnosis'], axis=1)\n",
        "y = bc_df['diagnosis']\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "-SWn2iSQ2K_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=10\n",
        ")\n",
        "\n",
        "print('Train size : ', X_train.shape)\n",
        "print('Test size  : ', X_test.shape)"
      ],
      "metadata": {
        "id": "umzDxQ_t2bqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D- Handling Outlier"
      ],
      "metadata": {
        "id": "-eDprS7k2tU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E- Scaling"
      ],
      "metadata": {
        "id": "IzN9r5tr2xmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F- Encoding"
      ],
      "metadata": {
        "id": "23TeCRC920W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded = X_train.copy()\n",
        "X_test_encoded = X_test.copy()"
      ],
      "metadata": {
        "id": "p62C_27B20xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Encoding - Target\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "\n",
        "y_train_encoded = le.transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "y_train_encoded"
      ],
      "metadata": {
        "id": "OWiZJfQM4gcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Class Names into a Variable\n",
        "classes = le.classes_\n",
        "classes"
      ],
      "metadata": {
        "id": "YVPSuI9Y4ud-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## G- Balancing Data"
      ],
      "metadata": {
        "id": "vzCsWhZh4-FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Training Model_"
      ],
      "metadata": {
        "id": "efCEuqQv5Bcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training using Decision Tree\n",
        "%%time\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dt = DecisionTreeClassifier(max_depth=6, random_state=10)\n",
        "model_dt.fit(X_train_encoded, y_train_encoded)"
      ],
      "metadata": {
        "id": "wQwiQYkn5DWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training using KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Membuat model KNN dengan jumlah tetangga = 10\n",
        "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# Melatih model menggunakan data training yang sudah di-encode\n",
        "knn_model.fit(X_train_encoded, y_train_encoded)"
      ],
      "metadata": {
        "id": "TPsIgC2xPOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Hyperparameter Tunning_"
      ],
      "metadata": {
        "id": "rxHR-zbrR109"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation using `cross_val_score` for Decision Tree\n",
        "%%time\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "dt_cross_val = cross_val_score(model_dt,\n",
        "                               X_train_encoded,\n",
        "                               y_train_encoded,\n",
        "                               cv=5,\n",
        "                               scoring=\"accuracy\")\n",
        "\n",
        "print('Decision Tree - Accuracy (All Folds) : ', dt_cross_val)\n",
        "print('Decision Tree - Mean Accuracy        : ', dt_cross_val.mean())\n",
        "print('Decision Tree - Std Dev              : ', dt_cross_val.std())\n",
        "print('Decision Tree - Range Test-Set       : ',\n",
        "      (dt_cross_val.mean() - dt_cross_val.std()), '-',\n",
        "      (dt_cross_val.mean() + dt_cross_val.std()))"
      ],
      "metadata": {
        "id": "4UYbjwnzSFBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap inisial dari prosedur cross-validation, kinerja model Decision Tree menunjukkan hasil yang memuaskan dengan nilai rata-rata akurasi sebesar 92%. Meskipun demikian, nilai standar deviasi yang teramati, yakni sekitar 0.033, mengindikasikan adanya fluktuasi minor pada hasil yang diperoleh di setiap fold. Ini menyiratkan bahwa kestabilan model belum mencapai level optimal. Rentang akurasi model berada pada 88,7% hingga 95,3%, yang menegaskan bahwa meskipun model dapat mencapai performa yang sangat tinggi pada beberapa fold, terjadi penurunan akurasi pada fold yang lain. Variabilitas performa ini dianggap sebagai kondisi yang dapat dimaklumi sebelum implementasi tuning (penyetelan parameter) dilakukan."
      ],
      "metadata": {
        "id": "gORuSIokfilQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation using `cross_val_score` for KNN\n",
        "%%time\n",
        "knn_cross_val = cross_val_score(knn_model,\n",
        "                                X_train_encoded,\n",
        "                                y_train_encoded,\n",
        "                                cv=5,\n",
        "                                scoring=\"accuracy\")\n",
        "\n",
        "print('KNN - Accuracy (All Folds)           : ', knn_cross_val)\n",
        "print('KNN - Mean Accuracy                  : ', knn_cross_val.mean())\n",
        "print('KNN - Std Dev                        : ', knn_cross_val.std())\n",
        "print('KNN - Range Test-Set                 : ',\n",
        "      (knn_cross_val.mean() - knn_cross_val.std()), '-',\n",
        "      (knn_cross_val.mean() + knn_cross_val.std()))"
      ],
      "metadata": {
        "id": "BTZ1rN70SMkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sebelum** tuning, KNN sudah memiliki performa yang baik dengan rata-rata akurasi 92,48%.\n",
        "Standar deviasi 0.016 jauh lebih kecil dibanding Decision Tree, menunjukkan bahwa KNN lebih stabil antar-fold.\n",
        "Rentang akurasi berada di 90,9% – 94,1%, yang berarti performa KNN cukup konsisten meskipun belum optimum."
      ],
      "metadata": {
        "id": "UoiApapdfwC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Parameter space untuk Decision Tree\n",
        "param_dist_dt = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "    'max_depth': randint(2, 50),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 20),\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "random_search_dt = RandomizedSearchCV(\n",
        "    estimator=model_dt,\n",
        "    param_distributions=param_dist_dt,\n",
        "    n_iter=100,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search_dt.fit(X_train_encoded, y_train_encoded)\n",
        "print(\"Parameter terbaik untuk Decision Tree:\", random_search_dt.best_params_)"
      ],
      "metadata": {
        "id": "qkfupy7TSg7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Parameter space untuk KNN\n",
        "param_dist_knn = {\n",
        "    'n_neighbors': randint(3, 500),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['minkowski', 'manhattan', 'mahalanobis', 'euclidean', 'hamming'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'leaf_size': randint(10, 60)\n",
        "}\n",
        "\n",
        "random_search_knn = RandomizedSearchCV(\n",
        "    estimator=knn_model,\n",
        "    param_distributions=param_dist_knn,\n",
        "    n_iter=100,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search_knn.fit(X_train_encoded, y_train_encoded)\n",
        "\n",
        "print(\"Parameter terbaik untuk KNN:\", random_search_knn.best_params_)\n"
      ],
      "metadata": {
        "id": "41z5KB44SxTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# menampilkan nilai CV untuk Decision Tree\n",
        "results_dt = random_search_dt.cv_results_\n",
        "best_index_dt = random_search_dt.best_index_\n",
        "n_splits_dt = random_search_dt.n_splits_\n",
        "\n",
        "accuracy_cross_val_dt = np.array([\n",
        "    results_dt[f'split{i}_test_score'][best_index_dt] for i in range(n_splits_dt)\n",
        "])\n",
        "\n",
        "print(\"Parameter Terbaik (Decision Tree):\", random_search_dt.best_params_)\n",
        "print(\"-\" * 40)\n",
        "print('accuracy - All - Cross Validation  : ', accuracy_cross_val_dt)\n",
        "print('accuracy - Mean - Cross Validation : ', accuracy_cross_val_dt.mean())\n",
        "print('accuracy - Std - Cross Validation  : ', accuracy_cross_val_dt.std())\n",
        "print('accuracy - Range of Test-Set       : ',\n",
        "      (accuracy_cross_val_dt.mean() - accuracy_cross_val_dt.std()),\n",
        "      '-',\n",
        "      (accuracy_cross_val_dt.mean() + accuracy_cross_val_dt.std()))\n"
      ],
      "metadata": {
        "id": "OiVEHYNrTule"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada fase sebelum tuning parameter, model K-Nearest Neighbors (KNN) telah menunjukkan kinerja yang baik dengan rata-rata akurasi sebesar 92,48%. Nilai standar deviasi yang tercatat, yaitu 0.016, secara signifikan lebih kecil jika dibandingkan dengan model Decision Tree, mengindikasikan bahwa KNN memiliki tingkat stabilitas yang lebih tinggi antar-fold. Selain itu, rentang akurasi yang berada di antara 90,9% hingga 94,1% menunjukkan bahwa performa KNN sudah cukup konsisten, meskipun hasil tersebut belum merepresentasikan kinerja yang optimum (terbaik) sebelum penyetelan parameter dilakukan."
      ],
      "metadata": {
        "id": "Crz7lVk7gBaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# menampilkan nilai CV untuk KNN\n",
        "results_knn = random_search_knn.cv_results_\n",
        "best_index_knn = random_search_knn.best_index_\n",
        "n_splits_knn = random_search_knn.n_splits_\n",
        "\n",
        "accuracy_cross_val_knn = np.array([\n",
        "    results_knn[f'split{i}_test_score'][best_index_knn] for i in range(n_splits_knn)\n",
        "])\n",
        "\n",
        "print(\"Parameter Terbaik (KNN):\", random_search_knn.best_params_)\n",
        "print(\"-\" * 40)\n",
        "print('accuracy - All - Cross Validation  : ', accuracy_cross_val_knn)\n",
        "print('accuracy - Mean - Cross Validation : ', accuracy_cross_val_knn.mean())\n",
        "print('accuracy - Std - Cross Validation  : ', accuracy_cross_val_knn.std())\n",
        "print('accuracy - Range of Test-Set       : ',\n",
        "      (accuracy_cross_val_knn.mean() - accuracy_cross_val_knn.std()),\n",
        "      '-',\n",
        "      (accuracy_cross_val_knn.mean() + accuracy_cross_val_knn.std()))"
      ],
      "metadata": {
        "id": "dY2UQnJRTyKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meskipun telah dilakukan proses tuning parameter, rata-rata akurasi model K-Nearest Neighbors (KNN) tetap berada pada kisaran 92,48%, yang mengindikasikan bahwa tidak terjadi peningkatan kinerja yang signifikan. Kondisi ini menyiratkan bahwa model KNN pada dasarnya telah beroperasi pada kondisi yang cukup optimal sebelum proses penyetelan, sehingga modifikasi pada parameter tidak menghasilkan peningkatan substansial. Namun, perlu dicatat bahwa standar deviasi mengalami peningkatan dari 0.016 menjadi 0.025. Peningkatan ini menunjukkan bahwa stabilitas model sedikit menurun (menjadi kurang stabil) pasca tuning. Perubahan ini juga tercermin pada rentang akurasi yang melebar menjadi 89,9% hingga 95,0%, yang sedikit lebih luas dibandingkan rentang yang diamati sebelumnya."
      ],
      "metadata": {
        "id": "jLCD2u_QgGLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Model - Decision Tree\n",
        "\n",
        "model_dt_final = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=6,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    splitter='best',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_dt_final.fit(X_train_encoded, y_train_encoded)"
      ],
      "metadata": {
        "id": "5JMo0JgSUDd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Model - KNN\n",
        "\n",
        "model_knn_final = KNeighborsClassifier(\n",
        "    n_neighbors=43,\n",
        "    weights='distance',\n",
        "    metric='manhattan',\n",
        "    algorithm='ball_tree',\n",
        "    leaf_size=48,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model_knn_final.fit(X_train_encoded, y_train_encoded)"
      ],
      "metadata": {
        "id": "54Ta9hqvUE-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Model Performance_"
      ],
      "metadata": {
        "id": "2TfAEUqv5_Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "def performance_check(clf, X, y, classes):\n",
        "    y_pred = clf.predict(X)\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "    print(classification_report(y, y_pred, target_names=classes))\n",
        "\n",
        "print('Decision Tree - Train')\n",
        "performance_check(model_dt, X_train_encoded, y_train_encoded, classes)\n",
        "print('')\n",
        "\n",
        "print('Decision Tree - Test')\n",
        "performance_check(model_dt, X_test_encoded, y_test_encoded, classes)\n"
      ],
      "metadata": {
        "id": "DswLjYFn6BUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Decision Tree yang dikembangkan menunjukkan akurasi sempurna (100%) pada data train dan akurasi sebesar 95% pada data test. Selisih persentase ini, yang mengindikasikan adanya sedikit penurunan kinerja pada data yang belum pernah dilihat sebelumnya, menyarankan potensi terjadinya overfitting ringan. Meskipun demikian, akurasi 95% yang dicapai pada data test secara substansial membuktikan bahwa model memiliki performa yang baik dan kapabilitas generalisasi yang kuat dalam mengklasifikasikan data baru."
      ],
      "metadata": {
        "id": "6X6J4qxeh0Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation untuk KNN\n",
        "\n",
        "def performance_check_knn(clf, X, y, classes):\n",
        "    y_pred = clf.predict(X)\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "    disp.plot()\n",
        "    plt.show()\n",
        "    print(classification_report(y, y_pred, target_names=classes))\n",
        "\n",
        "print('KNN - Train')\n",
        "performance_check(knn_model, X_train_encoded, y_train_encoded, classes)\n",
        "print('')\n",
        "\n",
        "print('KNN - Test')\n",
        "performance_check(knn_model, X_test_encoded, y_test_encoded, classes)\n"
      ],
      "metadata": {
        "id": "lZ_k4r0SV6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model K-Nearest Neighbors (KNN) yang dievaluasi mencapai akurasi sebesar 94% pada data train dan 93% pada data test. Perbedaan persentase yang minimal ini mengindikasikan bahwa model memiliki stabilitas yang tinggi dan tidak menunjukkan gejala overfitting, sehingga mampu mempertahankan kinerja secara konsisten pada data baru. Secara keseluruhan, performa klasifikasi model ini dinilai baik dan konsisten, meskipun akurasinya tercatat sedikit lebih rendah dibandingkan dengan model Decision Tree."
      ],
      "metadata": {
        "id": "w6cRwUyAiD3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - Decision Tree\n",
        "\n",
        "print('Decision Tree - Train')\n",
        "performance_check(model_dt_final, X_train_encoded, y_train_encoded, classes)\n",
        "print('')\n",
        "\n",
        "print('Decision Tree - Test')\n",
        "performance_check(model_dt_final, X_test_encoded, y_test_encoded, classes)"
      ],
      "metadata": {
        "id": "4bErdlT2YkOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Decision Tree final menunjukkan peningkatan performa yang signifikan dibandingkan versi sebelumnya, mencapai akurasi sebesar 99% pada data train dan 97% pada data test. Tingkat akurasi yang tinggi pada kedua set data ini menegaskan bahwa model memiliki kapabilitas yang superior dalam menangkap pola yang mendasari data tanpa menunjukkan gejala overfitting. Oleh karena itu, model ini dapat diandalkan untuk memberikan prediksi yang handal dan menunjukkan generalisasi yang kuat pada data yang belum pernah dilihat sebelumnya."
      ],
      "metadata": {
        "id": "Etdn-7mLiP3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - KNN\n",
        "\n",
        "print('KNN - Train')\n",
        "performance_check(model_knn_final, X_train_encoded, y_train_encoded, classes)\n",
        "print('')\n",
        "\n",
        "print('KNN - Test')\n",
        "performance_check(model_knn_final, X_test_encoded, y_test_encoded, classes)"
      ],
      "metadata": {
        "id": "MvbqYAlvYp_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model K-Nearest Neighbors (KNN) final mencapai akurasi sempurna (100%) pada data train dan 92% pada data test. Kesenjangan yang signifikan antara kedua nilai ini mengindikasikan adanya fenomena overfitting, di mana model terlalu spesifik dalam mempelajari pola pada data train sehingga kapabilitas generalisasi dan performa pada data baru mengalami penurunan. Meskipun demikian, akurasi 92% pada data test masih dapat dikategorikan sebagai kinerja yang cukup baik, walaupun performa tersebut dinilai kurang optimal jika dibandingkan dengan hasil yang dicapai oleh model Decision Tree final."
      ],
      "metadata": {
        "id": "PNveMlYvC7OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Scikit-Learn and Graphviz\n",
        "\n",
        "import graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "dot_data = tree.export_graphviz(\n",
        "    model_dt,\n",
        "    out_file=None,\n",
        "    feature_names=X.columns.tolist(),\n",
        "    class_names=['Benign', 'Malignant'],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph\n"
      ],
      "metadata": {
        "id": "9SboECoZ68u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisasi pohon keputusan menunjukkan bahwa node yang berwarna oranye merepresentasikan mayoritas data berlabel Benign, sementara node berwarna biru merepresentasikan mayoritas data berlabel Malignant. Observasi penting adalah banyaknya node daun yang memiliki nilai Gini sama dengan 0. Hal ini mengindikasikan tingkat kemurnian (purity) data yang sangat tinggi pada node tersebut, yang berarti bahwa instansi data di dalamnya telah berhasil dipisahkan dan diklasifikasikan dengan sempurna. Secara keseluruhan, struktur pohon yang dihasilkan menegaskan bahwa model memiliki kapabilitas yang efektif dalam membedakan dan mengklasifikasikan kedua kelas (Benign dan Malignant) dengan baik."
      ],
      "metadata": {
        "id": "W5GZ4NNOHIIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kesimpulan"
      ],
      "metadata": {
        "id": "cwtjwcNH7GPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan hasil evaluasi komprehensif, model Decision Tree (DT) terbukti menunjukkan kinerja superior untuk dataset Breast Cancer (Diagnostic) dibandingkan dengan model K-Nearest Neighbors (KNN). Secara spesifik, model DT final mencapai akurasi yang tinggi dan seimbang, yaitu 99% pada data train dan 97% pada data test. Kinerja cross-validation DT juga mengalami peningkatan pasca tuning, mencapai rata-rata akurasi 94,6% dengan standar deviasi 0,028, menegaskan kemampuan generalisasi yang kuat serta stabilitas antar-fold yang baik.\n",
        "\n",
        "Sebaliknya, meskipun model KNN final mencapai akurasi sempurna (100%) pada data train, akurasi pada data test hanya mencapai 92%. Kesenjangan yang signifikan ini mengindikasikan adanya masalah overfitting. Lebih lanjut, performa cross-validation KNN yang stabil sebelum tuning (rata-rata 92,48%, standar deviasi 0,016) tidak mengalami peningkatan signifikan setelah tuning; bahkan, proses tuning justru meningkatkan standar deviasi menjadi 0,025, yang mencerminkan penurunan kecil pada stabilitas model. Oleh karena itu, disimpulkan bahwa Decision Tree adalah model yang lebih optimal dan handal dalam mengekstraksi pola data dan memberikan prediksi yang konsisten pada data baru untuk dataset ini dibandingkan KNN."
      ],
      "metadata": {
        "id": "tFzoWVfz7Jsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referensi"
      ],
      "metadata": {
        "id": "gKM-G0yv7JeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nurani, A. T., Setiawan, A., & Susanto, B. (2023). Perbandingan kinerja regresi decision tree dan regresi linear berganda untuk prediksi bmi pada dataset asthma. *Jurnal Sains dan Edukasi Sains, 6*(1), 34-43.\n",
        "\n",
        "Pratiwi, R., Hayati, M. N., & Prangga, S. (2020). Perbandingan Klasifikasi Algoritma C5. 0 Dengan Classification and Regression Tree (Studi Kasus: Data Sosial Kepala Keluarga Masyarakat Desa Teluk Baru Kecamatan Muara Ancalong Tahun 2019). BAREKENG: *Jurnal Ilmu Matematika dan Terapan, 14*(2), 267-278.\n",
        "\n",
        "Maylita, N. M. S., Zahro, H. Z., & Vendyansyah, N. (2022). Penerapan Metode K-Nearest Neighbor (KNN) Untuk Menentukan Status Gizi Balita. JATI *(Jurnal Mahasiswa Teknik Informatika), 6*(2), 953-956.\n"
      ],
      "metadata": {
        "id": "rvvPvLW97Oso"
      }
    }
  ]
}